{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPv1Pewawbmvy7JZPwr53+P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XavierXinchi/Slot-Extraction-from-MultiWOZ/blob/main/cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XIApOePaDZT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/FYP')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install nltk\n",
        "!pip install keybert"
      ],
      "metadata": {
        "id": "W-OviRh3amJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from keybert import KeyBERT\n",
        "import re\n",
        "\n",
        "batch_size = 16;\n",
        "# Loading pre-trained BERT models using the huggingface library\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# read tsv file\n",
        "def read_tsv(file_name):\n",
        "    data = []\n",
        "    with open(file_name, 'r', encoding='utf-8') as tsv_file:\n",
        "        reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            data.append(row)\n",
        "    return data\n",
        "\n",
        "# create dictionary\n",
        "def create_dict(data):\n",
        "    domain_dict = defaultdict(list)\n",
        "    for row in data:\n",
        "        dialogue_id, speaker, utterance, domain_list = row\n",
        "        domain_list = domain_list.strip('][').split(\n",
        "            ', ')  # convert string to list\n",
        "        if speaker == 'USER':\n",
        "            for domain in domain_list:\n",
        "                cleaned_domain = domain.strip(\"'\")  # remove single quotes\n",
        "                domain_dict[cleaned_domain].append(utterance)\n",
        "    return domain_dict\n",
        "  \n",
        "def extract_key_phrases(embeddings, utterances, num_keywords=2, batch_size=16):\n",
        "    key_model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
        "    key_phrases = []\n",
        "    num_utterances = len(utterances)\n",
        "    \n",
        "    # Create batches of utterances\n",
        "    utterance_batches = [utterances[i:i + batch_size] for i in range(0, num_utterances, batch_size)]\n",
        "    \n",
        "    for batch in tqdm(utterance_batches, desc=\"Extracting key phrases\"):\n",
        "        for utterance in batch:\n",
        "            keywords = key_model.extract_keywords(\n",
        "                utterance, keyphrase_ngram_range=(1, 1), stop_words=None, use_maxsum=True, nr_candidates=20, top_n=num_keywords)\n",
        "            key_phrases.append([kw[0] for kw in keywords])\n",
        "    \n",
        "    return key_phrases\n",
        "\n",
        "def generate_slot_names(clustered_domains, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for domain, clusters in clustered_domains.items():\n",
        "            slot_names = []\n",
        "            for cluster_label, cluster_info in clusters.items():\n",
        "                # Extract key phrases for each cluster\n",
        "                embeddings = cluster_info['embeddings']\n",
        "                utterances = cluster_info['utterances']\n",
        "                key_phrases = extract_key_phrases(embeddings, utterances)\n",
        "\n",
        "                # Flatten the list of key phrases\n",
        "                flat_key_phrases = [phrase for phrases in key_phrases for phrase in phrases]\n",
        "\n",
        "                # Count and sort the key phrases\n",
        "                phrase_count = Counter(flat_key_phrases)\n",
        "                sorted_phrases = sorted(phrase_count.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                # Remove the most frequent words\n",
        "                slot_name = ' '.join([phrase[0] for phrase in sorted_phrases[1:]])\n",
        "                slot_names.append(slot_name)\n",
        "\n",
        "            f.write(f\"{domain}\\t{' '.join(slot_names)}\\n\")\n",
        "\n",
        "def cluster_utterances(domain_dict):\n",
        "    clustered_domains = {}\n",
        "\n",
        "    # 1. Check if there are CUDA devices available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 2. Move the model to GPU (if available)\n",
        "    model.to(device)\n",
        "    \n",
        "    for domain, utterances in tqdm(domain_dict.items(), desc=\"Clustering domains\"):\n",
        "        # Create batches of encoded utterances\n",
        "        batches = []\n",
        "        for i in range(0, len(utterances), batch_size):\n",
        "            batch = utterances[i:i + batch_size]\n",
        "            encoded_batch = tokenizer(\n",
        "                batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            batches.append(encoded_batch)\n",
        "\n",
        "        embeddings = []\n",
        "        for encoded_utterances in batches:\n",
        "            # Move input data to GPU (if available)\n",
        "            encoded_utterances = {key: value.to(\n",
        "                device) for key, value in encoded_utterances.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                batch_embeddings = model(\n",
        "                    **encoded_utterances).last_hidden_state[:, 0, :].cpu().numpy()\n",
        "                embeddings.extend(batch_embeddings)\n",
        "\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        n_clusters = 2\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        clustered_domains[domain] = defaultdict(\n",
        "            lambda: {'utterances': [], 'embeddings': []})\n",
        "\n",
        "        for i, label in enumerate(labels):\n",
        "            clustered_domains[domain][label]['utterances'].append(\n",
        "                utterances[i])\n",
        "            clustered_domains[domain][label]['embeddings'].append(\n",
        "                embeddings[i])\n",
        "\n",
        "        # Compute word distribution for each cluster in each domain\n",
        "        for cluster_label, cluster_info in clustered_domains[domain].items():\n",
        "            word_count = defaultdict(int)\n",
        "            total_words = 0\n",
        "            cluster_utterances = cluster_info['utterances']\n",
        "\n",
        "            for utterance in cluster_utterances:\n",
        "                # Extract words without punctuation and convert to lowercase\n",
        "                words = [word.lower() for word in re.findall(r'\\b[a-zA-Z]+\\b', utterance)]\n",
        "                for word in words:\n",
        "                    if word.lower() not in stop_words:  # Only count non-stopwords\n",
        "                        word_count[word] += 1\n",
        "                        total_words += 1\n",
        "\n",
        "            # Sort word distribution by frequency in descending order\n",
        "            sorted_word_distribution = {\n",
        "                word: count / total_words for word, count in sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "            }\n",
        "            clustered_domains[domain][cluster_label]['word_distribution'] = sorted_word_distribution\n",
        "            clustered_domains[domain][cluster_label]['total_words'] = total_words  # Store total words in the cluster\n",
        "\n",
        "    return clustered_domains\n",
        "\n",
        "\n",
        "def main():\n",
        "    file_name = 'FYP/MyDrive/Colab Notebooks/FYP/all_dialogues_utterances.tsv'\n",
        "    data = read_tsv(file_name)\n",
        "    domain_dict = create_dict(data)\n",
        "    clustered_domains = cluster_utterances(domain_dict)\n",
        "    output_file = \"slot_names.tsv\"\n",
        "    generate_slot_names(clustered_domains, output_file)\n",
        "\n",
        "    # Print Clustering Results\n",
        "    for domain, clusters in clustered_domains.items():\n",
        "        print(f\"Domain: {domain}\")\n",
        "        for cluster_label, cluster_info in clusters.items():\n",
        "            print(f\"\\tCluster {cluster_label}:\")\n",
        "            print(f\"\\tTotal Words: {cluster_info['total_words']}\")  # Print total words in the cluster\n",
        "            print(\"\\tUtterances:\")\n",
        "            for utterance in cluster_info['utterances']:\n",
        "                print(f\"\\t\\t{utterance}\")\n",
        "            print(\"\\tWord Distribution:\")\n",
        "            for word, distribution in cluster_info['word_distribution'].items():\n",
        "                print(f\"\\t\\t{word}: {distribution}\")\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "9w8tBwIGbKfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8moYtLh1Yek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}